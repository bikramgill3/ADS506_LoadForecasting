---
title: "ADS506 - Group Project - ERCOT Load Forecasting"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
library(astsa)
library(ggplot2)
library(readxl)
library(tidyverse)
library(ggdark)
library(reshape2) 
library(xts)
library(dplyr)
library(EnvStats)
library(forecast)
library(rugarch)
```

Please see code blocks below which have been used to load, prep, transform and plot our data.

This will serve as our initial pre-processing, which we can further analyze for our modelling phase and our paper.

Further analysis will also be performed as we dive deeper into our paper and final code. 


```{r dataload}
load17.df <- read_excel('Native_Load_2017.xlsx')
load18.df <- read_excel('Native_Load_2018.xlsx')
load19.df <- read_excel('Native_Load_2019.xlsx')
load20.df <- read_excel('Native_Load_2020.xlsx')
load21.df <- read_excel('Native_Load_2021.xlsx')
```


```{r dataprep}
## The load dataset for 2017 had a space in the Timestamp column, preventing the binding of the four years
colnames(load17.df)[1] <- "HourEnding"

## Using bind_rows from tidyverse, the data from 2017-2020 was merged.
load_4_year <- bind_rows(load17.df,load18.df,load19.df,load20.df)
sum(is.na(load_4_year))

load_4_year <- load_4_year[c("HourEnding","ERCOT")]



## Using strptime, the time variable data type was set. 
load_4_year$HourEnding <- as.POSIXct(strptime(load_4_year$HourEnding, format="%m/%d/%Y %H:%M"))

#Remove NA's which arise due to issues with daylight savings (1 record in each year)
load_4_year <- na.omit(load_4_year)

## Converting to a time series
ts_load <- xts(load_4_year, load_4_year$HourEnding)
ts_load <- ts_load[,-1]
head(ts_load)
```



It is clear that a min max normalization scalar is not a usable option as it does not account for the mean. The graph is still visually the same.


```{r bg_arima}

##### Bikram: notes here; im going to plot the P/ACF's, which should show if we need to difference, AR(x) and MA(x).

### From there, will fit the ARIMA model, forecast 2021 and then plot the dataset vs forecast
library(ggplot2)
library(ggfortify)

# cbind("Raw Data" = load_4_year$ERCOT,
#       "Logs" = log(load_4_year$ERCOT),
#       "Seasonally\n differenced logs" =
#         diff(log(load_4_year$ERCOT),24),
#       "Doubly\n differenced logs" =
#         diff(diff(log(load_4_year$ERCOT),24),1)) %>%
#   autoplot(facets=TRUE) +
#     xlab("Year") + ylab("") +
#     ggtitle("Monthly US net electricity generation")


par(mfrow = c(4,1)) 
acf2(load_4_year$ERCOT, main = "Raw Data")
acf2(diff(load_4_year$ERCOT), main = "First-Order Difference")
acf2(diff(diff(load_4_year$ERCOT)), main = "Second-Order Difference")
acf2(diff(log(load_4_year$ERCOT),24), main = "Seasonally differenced logs")

### We see clear cyclicality in the raw data, first-order difference and second order difference. 
### When observing the differences, the trend can be seen around lag 24, indiciating this is a daily autocorrelatioj (e.g. power demand at any given hour today will be highly correlated to the demand at the same time 24, 48, 72 hours ago etc.)

## Seasonally differencing produces a series that can be modelled.
### Based on the seasonally differenced P/ACF, the ACF tails off while the P/ACF cuts off. This indicates an AR(p) model. 

```

```{r bg_arima2}

### Overall, our model is the following:

### AR(24), Differenced(24), annd logged

log_load <- na.omit(log(load_4_year$ERCOT))

fit <- Arima(log_load, order=c(24,24,0))  ## getting an error on the fit

```

```{r GARCH Model}

# Model specification of a GARCH constant model
garchSpec = ugarchspec(
           variance.model=list(model="sGARCH",
                               garchOrder=c(1,1)),
           mean.model=list(arimaorder=c(24,24,0)), 
           distribution.model="std")

# Fitting the model
garchFit = ugarchfit(spec=garchSpec, data=log_load, out.sample = 20)
garchFit


# Plot the different useful graphs for the model estimation
plot(garchFit, which="all")
```

***Model Comments and Findings***

*The first table on the first part of the estimation results (Refer to "Optimal parameters" table) shows the optimal estimated parameters. Which means the significance of the estimated parameter.* 

*The second table (refer to "Information criteria"). Shows the Akaike (AIC), Bayes (BIC), Shibata and Hannan-Quinn criteria for the model estimation. Basically the lower these values, the better the model is in terms of fitting. And we can see the results we have are in the (-4) range, which is good.*


*The third table presents the Ljung-Box test for testing the serial correlation of the error terms. The null hypothesis states there is no serial correlation of the error terms. Basically, if the p-value is lower than 5%, the null hypothesis is rejected. Our results show that our p-value is higher than 5% on the on "Standardized Squared Residuals", meaning that there is no serial correlation of the error term.*


*The last table is yet another interesting one to check (refer to "Adjusted Pearson Goodness of Fit"), it measures the goodness of fit of the error. The null hypothesis says that the conditional error term follows a normal distribution. If the p-value is lower than 5%, the null hypothesis is rejected. Our results show three of the p-value higher than 5% and one lower.*


```{r}

```

